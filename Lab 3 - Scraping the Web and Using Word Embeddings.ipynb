{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## <center> Lab 3: Web Scraping and Using a Word Embedding </center>##\n",
    "\n",
    "#### <center> Working without APIs to get text and then one way of analyzing it.</center> ####\n",
    "<br><br>\n",
    "This notebook first works through a simple example of webscraping in python. APIs are the best way to get data, but sometimes they don't exist. Instead, you get the information by requesting the webpages your internet browser would get and design a simple program to pull out the useful parts. Often you'll actually be getting pages and dismantling them to find what pages to ask for next. Doing this is a form of what people call crawling, figuring out what data is out there and where is it because nobody is going to be able to, or want to, tell you the answer.<br> \n",
    "<br>\n",
    "You'll start to understand webscraping requires you to be scrappy because there is no universal way of making a webpage. You have to reverse engineer what the web designer/programmer did to pack the underlying data into the nice formatting you see in a web browser. You'll learn there is a lot packed into the HTML document that becomes the page you see. We'll only just touch the surface of web design and what you might have to do to get the data you want. <br><br>\n",
    "\n",
    "The second part of the lab is the analysis of our scraped text using a word embedding. This can start to uncover patterns in the text. A word embedding is a projection of a vocabulary into a high (often 300) dimensional space so that we can explore the \"spatial\" relationships between words. You can create your own word embedding, but you need far more input data than what we'll be collecting. Instead, we'll be using an existing word embedding to look for relationships in Bob Dylan's lyrics. This is only one way of analyzing text quantitatively, but it is a great place to start because it introduces many of the concepts and is tractable. \n",
    "<br><br>\n",
    "\n",
    "<h4>Bob Dylan's Lyrics</h4>\n",
    "To learn the basics of scraping, we are going to download all of Bob Dylan's lyrics, which someone has conveniently posted on a single website. Other than being a fairly straightforward example of scraping, it <em>might</em> be interesting to explore the lyrics through a word embedding.\n",
    "<br>\n",
    "<br>\n",
    "First we import the module/package `Requests`, which will allow us to get webpages from the internet; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're missing a module/package you'll need to get it. The easiest way is from the terminal/command line. If you're running python 2.7 use `pip`. If you're running python 3 use `pip3`. If you're using Anaconda, you should have the package already.<br>\n",
    "Ex:<br>\n",
    "`pip install requests`<br>\n",
    "`pip3 install requests`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We supply the module's `get` function with the page we want, here <em><a href=http://bobdylan.com/songs>bobdylan.com/songs</a></em>. It will return the same page your internet browser would get. We store it in a variable. I named it `homepage`, but you can name it anything that is not a <a href=https://docs.python.org/2.5/ref/keywords.html>python keyword</a>. You just have to be consistent with whatever variables names you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "homepage = requests.get(\"http://bobdylan.com/songs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function actually returns a <em>Response</em> object, which contains metadata in addition to the HTML used to render the page. To get to just the HTML, we need to look at the `text` field of the response object, i.e. `homepage.text`\n",
    "\n",
    "If you look at this page in your browser, you'll see it is a long list of songs and not the lyrics themselves. That means we need to crawl this website to find the information we actually need. Crawling means creating a routine that identifies the location of the information we actually want so that we don't have to specify it ourselves. In this case, that means the links to the actual lyrics. We'll find those and then use them to get to the lyrics. The alternative is to create a list of links by hand (yuck).\n",
    "\n",
    "<br>\n",
    "If you print the `homepage.text` field, you'll see the HTML your browser uses to determine how to display the page. HTML is human-readable, but contains a lot of information we don't actually want. When printed without the proper visual formatting, it is downright disorienting. But the information we need is in there so we're going to dig it out!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "homepage.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>To be able to extract the information we actually want, we're going to use a module named `Beautiful Soup` to <em>parse</em> the HTML into something we can read and then approach computationally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll pass the HTML data in `homepage.text` to BeautifulSoup. We also need to specify a parser. Using `\"html.parser\"` is perfectly fine for 99.99% of use-cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(homepage.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>soup</code> object has presorted some commen elements of the HTML for us and gives us access to methods to find more complicated things. An example of the former is <code>soup.title</code>. Nearly all pages have a defined title element which your browser displays up on browser header, usually as the tab name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Songs | The Official Bob Dylan Site</title>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll note that the text of title element that shows up in your broswer here has tags, i.e. `<title></title>`. That means we haven't actually gotten down to raw text. Actually the great thing about BeautifulSoup; your results are BeautifulSoup object that can be searched again and again. <b>BUT</b> it means we need to sure we do actually get to the raw text once we find what we're looking for. The next command does that and highlights the ability of python to navigate object structures succinctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Songs | The Official Bob Dylan Site'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Next, we'll find the element with the hyperlinks we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "songs = soup.find_all(\"span\",\"song\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `find_all()` method with the parameters `\"span\"` and `\"song\"` finds all elements of type <em>span</em> with a class equal to <em>song</em>. The span element is a generic element type you'll find in a lot of HTML but the class <em>song</em> is custom to this website. So how did we know to look for span elements of the class song? I had to look at the raw HTML of the website and find examples of what I wanted and then figure out how the website developer encoded that information. This type of task is part of the black art of webscraping and can get much more complicated because developers might be inconsistent with their use of code or use more complex (and efficient) ways of passing data between their servers and your computer. Thankfully this website is quite easy to navigate. You could probably do it just looking at the `homepage.text` field above, but I'd recommend trying it in your browser. All the major browsers allow you to view the source document or find the thing that houses a particular part of the page. \n",
    "\n",
    "Firefox, Safari and Chrome all let you right click on a part of the page and \"Inspect Element\". This will open up a sidebar or another tab which jumps to and highlights the element you're viewing in the raw HTML. Elements are often nested in HTML (e.g. a cell in a table in a \"div\" in the body) and the browser doesn't necessarily highlight the lowest element. Accordingly you might need to expand the highlighted element to find what you're looking for. Try this on bobdylan.com/songs to see where the \"span\" and \"song\" parameters above come from.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "Now let's look at one these song elements. We'll look at the second because it turns out the first song's lyrics aren't on the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<span class=\"song\"><a href=\"http://bobdylan.com/songs/til-i-fell-love-you/\">‘Til I Fell In Love With You</a></span>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the hyperlink we want, but we need to dig it out before we store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "songs_hrefs = []\n",
    "\n",
    "for song in songs:\n",
    "    songs_hrefs.append(song.find(\"a\").get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we first created an empty list to store the hyperlinks we find. Then we iterate over the elements of `songs` and find the `<a>` (anchor) element and get the \"href\" variable from inside of it. We immediately put that in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "653"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(songs_hrefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see we've filled the list with 653 links to songs. Sure glad we didn't try to do that by hand!<Br><br>\n",
    "    We now have the links and can go back to get all those pages using the `requests` module. But first, let's just start with one so that we can figure out how to get the relevant information out of the response we'll get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "song_one = requests.get(songs_hrefs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you'll recall, the response object has metadata and then a bunch of raw HTML. We'll pull just the text and run it into BeautifulSoup to get something we can actually read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_soup = BeautifulSoup(song_one.text,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lyrics = new_soup.find(\"div\",\"article-content lyrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like for the home page, I looked at the raw HTML in order to figure out how the data we want is stored. It turned out to be in a `<div>` tag with a class named `article-content lyrics`<br><br>\n",
    "Let's print this to see what we've got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"article-content lyrics\">\n",
      "\t\t\tWell, my nerves are exploding and my body’s tense<br/>\r\n",
      "I feel like the whole world got me pinned up against the fence<br/>\r\n",
      "I’ve been hit too hard, I’ve seen too much<br/>\r\n",
      "Nothing can heal me now, but your touch<br/>\r\n",
      "I don’t know what I’m gonna do<br/>\r\n",
      "I was all right ’til I fell in love with you<br/>\n",
      "<br/>\r\n",
      "Well, my house is on fire, burning to the sky<br/>\r\n",
      "I thought it would rain but the clouds passed by<br/>\r\n",
      "Now I feel like I’m coming to the end of my way<br/>\r\n",
      "But I know God is my shield and he won’t lead me astray<br/>\r\n",
      "Still I don’t know what I’m gonna do<br/>\r\n",
      "I was all right ’til I fell in love with you<br/>\n",
      "<br/>\r\n",
      "Boys in the street beginning to play<br/>\r\n",
      "Girls like birds flying away<br/>\r\n",
      "When I’m gone you will remember my name<br/>\r\n",
      "I’m gonna win my way to wealth and fame<br/>\r\n",
      "I don’t know what I’m gonna do<br/>\r\n",
      "I was all right ’til I fell in love with you<br/>\n",
      "<br/>\r\n",
      "Junk is piling up, taking up space<br/>\r\n",
      "My eyes feel like they’re falling off my face<br/>\r\n",
      "Sweat falling down, I’m staring at the floor<br/>\r\n",
      "I’m thinking about that girl who won’t be back no more<br/>\r\n",
      "I don’t know what I’m gonna do<br/>\r\n",
      "I was all right ’til I fell in love with you<br/>\n",
      "<br/>\r\n",
      "Well, I’m tired of talking, I’m tired of trying to explain<br/>\r\n",
      "My attempts to please you were all in vain<br/>\r\n",
      "Tomorrow night before the sun goes down<br/>\r\n",
      "If I’m still among the living, I’ll be Dixie bound<br/>\r\n",
      "I just don’t know what I’m gonna do<br/>\r\n",
      "I was all right ’til I fell in love with you\t\t\t\t\t\t\t\t\t\t<p class=\"copytext\">Copyright © 1997 by Special Rider Music</p>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "print(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's looking pretty good, but there's still stuff in there we wouldn't want to have make it into our eventual analysis of the text. We'll get it out of here right now.<br><br>\n",
    "The main problem is the copyright text at the bottom. The exact text will be different for different songs, but thankfully the web developer put it in its own `<p>` (paragraph) element. We'll identify that text and deal with it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copyright = lyrics.find(\"p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we have to deal with it later is that `lyrics` is a BeautifulSoup object. This makes it easier to find things, but we can't modify it. So we find the copyright information and then extract the HTML text from the lyrics object using the `text` field of the object. The text string we get from that <b>is</b> modifiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Copyright © 1997 by Special Rider Music'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copyright.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lyrics_rawtext = lyrics.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us an actual text string.\n",
    "\n",
    "Now we'll use the python `replace` function to replace the copyright information with the empty string `\"\"`. The replace function creates a new object so we'll save it to a new variable named `lyrics_text`. Note we access the `copyright` soup object's text field to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lyrics_text = lyrics_rawtext.replace(copyright.text,\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you print this variable, it will look like we're close to done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t\tWell, my nerves are exploding and my body’s tense\r\n",
      "I feel like the whole world got me pinned up against the fence\r\n",
      "I’ve been hit too hard, I’ve seen too much\r\n",
      "Nothing can heal me now, but your touch\r\n",
      "I don’t know what I’m gonna do\r\n",
      "I was all right ’til I fell in love with you\n",
      "\r\n",
      "Well, my house is on fire, burning to the sky\r\n",
      "I thought it would rain but the clouds passed by\r\n",
      "Now I feel like I’m coming to the end of my way\r\n",
      "But I know God is my shield and he won’t lead me astray\r\n",
      "Still I don’t know what I’m gonna do\r\n",
      "I was all right ’til I fell in love with you\n",
      "\r\n",
      "Boys in the street beginning to play\r\n",
      "Girls like birds flying away\r\n",
      "When I’m gone you will remember my name\r\n",
      "I’m gonna win my way to wealth and fame\r\n",
      "I don’t know what I’m gonna do\r\n",
      "I was all right ’til I fell in love with you\n",
      "\r\n",
      "Junk is piling up, taking up space\r\n",
      "My eyes feel like they’re falling off my face\r\n",
      "Sweat falling down, I’m staring at the floor\r\n",
      "I’m thinking about that girl who won’t be back no more\r\n",
      "I don’t know what I’m gonna do\r\n",
      "I was all right ’til I fell in love with you\n",
      "\r\n",
      "Well, I’m tired of talking, I’m tired of trying to explain\r\n",
      "My attempts to please you were all in vain\r\n",
      "Tomorrow night before the sun goes down\r\n",
      "If I’m still among the living, I’ll be Dixie bound\r\n",
      "I just don’t know what I’m gonna do\r\n",
      "I was all right ’til I fell in love with you\t\t\t\t\t\t\t\t\t\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lyrics_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're actually not that close. It turns out that Jupyter notebooks use the HTML still in the text in order to format it for viewing when you use the `print()` command. Note the difference when you \"execute\" just the variable in the next line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t\tWell, my nerves are exploding and my body’s tense\r\n",
      "I feel like the whole world got me pinned up against the fence\r\n",
      "I’ve been hit too hard, I’ve seen too much\r\n",
      "Nothing can heal me now, but your touch\r\n",
      "I don’t know what I’m gonna do\r\n",
      "I was all right ’til I fell in love with you\n",
      "\r\n",
      "Well, my house is on fire, burning to the sky\r\n",
      "I thought it would rain but the clouds passed by\r\n",
      "Now I feel like I’m coming to the end of my way\r\n",
      "But I know God is my shield and he won’t lead me astray\r\n",
      "Still I don’t know what I’m gonna do\r\n",
      "I was all right ’til I fell in love with you\n",
      "\r\n",
      "Boys in the street beginning to play\r\n",
      "Girls like birds flying away\r\n",
      "When I’m gone you will remember my name\r\n",
      "I’m gonna win my way to wealth and fame\r\n",
      "I don’t know what I’m gonna do\r\n",
      "I was all right ’til I fell in love with you\n",
      "\r\n",
      "Junk is piling up, taking up space\r\n",
      "My eyes feel like they’re falling off my face\r\n",
      "Sweat falling down, I’m staring at the floor\r\n",
      "I’m thinking about that girl who won’t be back no more\r\n",
      "I don’t know what I’m gonna do\r\n",
      "I was all right ’til I fell in love with you\n",
      "\r\n",
      "Well, I’m tired of talking, I’m tired of trying to explain\r\n",
      "My attempts to please you were all in vain\r\n",
      "Tomorrow night before the sun goes down\r\n",
      "If I’m still among the living, I’ll be Dixie bound\r\n",
      "I just don’t know what I’m gonna do\r\n",
      "I was all right ’til I fell in love with you\t\t\t\t\t\t\t\t\t\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lyrics_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get all those *whitespaces* and *delimiters* out while preserving the structure of the lyrics. Whoever encoded the lyrics ends the lines with `\\r\\n`. We'll split everything at those points first and iterate over the resulting lines, clean the individual lines and then add them to list named `clean_lines`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_lines = []\n",
    "\n",
    "lines = lyrics_text.split(\"\\r\\n\")\n",
    "\n",
    "for line in lines:\n",
    "    clean_lines.append(line.strip().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last line we use `strip()` to strip non-alphanumeric characters off the line. We also split the line whereever there is whitespace using `split()`.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Well,', 'my', 'nerves', 'are', 'exploding', 'and', 'my', 'body’s', 'tense'], ['I', 'feel', 'like', 'the', 'whole', 'world', 'got', 'me', 'pinned', 'up', 'against', 'the', 'fence'], ['I’ve', 'been', 'hit', 'too', 'hard,', 'I’ve', 'seen', 'too', 'much'], ['Nothing', 'can', 'heal', 'me', 'now,', 'but', 'your', 'touch'], ['I', 'don’t', 'know', 'what', 'I’m', 'gonna', 'do'], ['I', 'was', 'all', 'right', '’til', 'I', 'fell', 'in', 'love', 'with', 'you'], ['Well,', 'my', 'house', 'is', 'on', 'fire,', 'burning', 'to', 'the', 'sky'], ['I', 'thought', 'it', 'would', 'rain', 'but', 'the', 'clouds', 'passed', 'by'], ['Now', 'I', 'feel', 'like', 'I’m', 'coming', 'to', 'the', 'end', 'of', 'my', 'way'], ['But', 'I', 'know', 'God', 'is', 'my', 'shield', 'and', 'he', 'won’t', 'lead', 'me', 'astray'], ['Still', 'I', 'don’t', 'know', 'what', 'I’m', 'gonna', 'do'], ['I', 'was', 'all', 'right', '’til', 'I', 'fell', 'in', 'love', 'with', 'you'], ['Boys', 'in', 'the', 'street', 'beginning', 'to', 'play'], ['Girls', 'like', 'birds', 'flying', 'away'], ['When', 'I’m', 'gone', 'you', 'will', 'remember', 'my', 'name'], ['I’m', 'gonna', 'win', 'my', 'way', 'to', 'wealth', 'and', 'fame'], ['I', 'don’t', 'know', 'what', 'I’m', 'gonna', 'do'], ['I', 'was', 'all', 'right', '’til', 'I', 'fell', 'in', 'love', 'with', 'you'], ['Junk', 'is', 'piling', 'up,', 'taking', 'up', 'space'], ['My', 'eyes', 'feel', 'like', 'they’re', 'falling', 'off', 'my', 'face'], ['Sweat', 'falling', 'down,', 'I’m', 'staring', 'at', 'the', 'floor'], ['I’m', 'thinking', 'about', 'that', 'girl', 'who', 'won’t', 'be', 'back', 'no', 'more'], ['I', 'don’t', 'know', 'what', 'I’m', 'gonna', 'do'], ['I', 'was', 'all', 'right', '’til', 'I', 'fell', 'in', 'love', 'with', 'you'], ['Well,', 'I’m', 'tired', 'of', 'talking,', 'I’m', 'tired', 'of', 'trying', 'to', 'explain'], ['My', 'attempts', 'to', 'please', 'you', 'were', 'all', 'in', 'vain'], ['Tomorrow', 'night', 'before', 'the', 'sun', 'goes', 'down'], ['If', 'I’m', 'still', 'among', 'the', 'living,', 'I’ll', 'be', 'Dixie', 'bound'], ['I', 'just', 'don’t', 'know', 'what', 'I’m', 'gonna', 'do'], ['I', 'was', 'all', 'right', '’til', 'I', 'fell', 'in', 'love', 'with', 'you']]\n"
     ]
    }
   ],
   "source": [
    "print(clean_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you can see that we now have the individual words of each line in a list. All of those lists are in one big list. We're also preserved the order the words appear in the lyrics. At this point though we can use python list *indexing* to get individual words. If you want the sixth word of the second line, just run the next line. (Remember that python list indices all start at zero, so the 6th element has an index number of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world\n"
     ]
    }
   ],
   "source": [
    "print(clean_lines[1][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could be done extracting data here, but because we don't know what type of analysis we'll end up doing let's also record the year the song was first copyrighted. We'll do this using \"regular expressions\", a way of expressing text characters abstractly. We'll look for the copyright year as a string of four numbers between 1000 and 2999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not the place to go into the details of \"regex\" (<b>reg</b>ular <b>ex</b>pressions), but note that we supply the copyright string we found earlier as the string being searched. We also convert the result from a string to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997\n"
     ]
    }
   ],
   "source": [
    "year = int(re.search(\"[1-2][0-9]{3}\",copyright.text).group(0))\n",
    "print(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we've found everything we need it the HTML documents. But we don't want to run through the above steps 653 times, so let's package everything up into a single function. We can then just pass the page to the function and it will spit out exactly what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lyric_cleaner(page):\n",
    "    soup = BeautifulSoup(requests.get(page).text,\"html.parser\")\n",
    "    lyrics = soup.find(\"div\",\"article-content lyrics\")\n",
    "    if lyrics.text.strip() != \"\":  \n",
    "        copyright = lyrics.find(\"p\")\n",
    "        lyrics_rawtext = lyrics.text\n",
    "        lyrics_text = lyrics_rawtext.replace(copyright.text,\"\")\n",
    "        lines = lyrics_text.split(\"\\r\\n\")\n",
    "        clean_lines = []\n",
    "        for line in lines:\n",
    "            clean_lines.extend(line.strip().split())\n",
    "        if copyright.text.strip() != \"\":   \n",
    "            year = int(re.search(\"[1-2][0-9]{3}\",copyright.text).group(0))\n",
    "        else:\n",
    "            year = None\n",
    "        return [year,clean_lines]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two important things to note about this function. First is that we're returning a list of two elements, a year and the lyrics. This keep this information together for later. Second is that we first check that the elements of the page we're looking at aren't empty. If they were empty, Python would raise an error and stop everything. We screen the potential cases out using `if` statements to prevent this.<br><br>\n",
    "Ok, now we're almost ready to gather and clean the pages/lyrics. But before we do, we import the `time` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do this so that we can <b><em>slow down</em></b> how quickly we do this whole process. We do this because if you send a hundreds of requests to a server basically all at once, you're going to get your IP address blacklisted. Websites don't like it if you ask for so much information that it ties up their servers. To avoid doing this, we slow things down. This ensures the success of our scraping but is also courteous. The time module's sleep function pauses the procedure for the number of seconds you tell it to.  1 second is sufficient for what we're doing, but if you're dealing with a major site and getting lots of data, you might increase the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lyrics = {}\n",
    "lyrics_and_years = {}\n",
    "\n",
    "for page in songs_hrefs[:3]:\n",
    "    # the second to last item in split list is the name of the song\n",
    "    song_name = page.split(\"/\")[-2]\n",
    "    \n",
    "    cleaned_data = lyric_cleaner(page)\n",
    "    if cleaned_data != None:\n",
    "        lyrics_and_years[song_name] = cleaned_data\n",
    "        lyrics[song_name] = cleaned_data[1]\n",
    "    time.sleep(1) #sleeping for 1 second\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Couple of things:<br>\n",
    "We created two lists. We use `lyrics` to gather just the words of the songs. We <em>extend</em> the list each time. If you *extend* a list with a list, you get a flat list back (e.g. `[\"a\", \"list\"].extend([\"plus\",\"a\",\"list\"]) = [\"a\", \"list\", plus\",\"a\",\"list\"]`. If you *append* a list to a list, the outer list will contain a list (e.g. `[\"a\",\"list\"].append([\"plus\",\"a\",\"list\"]) = [\"a\",\"list\",[\"plus\",\"a\",\"list\"]]`. The package for doing word embeddings we are going to check out wants just a list of lists so in the end of this routine `lyrics` is one long list of lists where each inner list is the sequence of words from a song. This preserves the discrete object of the song which allows for a richer context for each word as the statistical structure is analyzed. This is different from the \"bag of words\" approach topic modeling uses and one the more appealling features of word embeddings.\n",
    "\n",
    "<br><br>\n",
    "Back to what we're done: We saved the song and copyright years together in a list of lists. We'll keep that list for later because knowing the contents of songs and when he wrote them might be a fruitful avenue for analysis.\n",
    "<br><br>\n",
    "Finally, the code above retrieves only the first three songs from our list of links because we \"sliced\" the list. `songs_hrefs[:3]` means take the first three elements of the list. I did this so that you don't accidentally run the code for all 653 songs, which takes 10+ minutes to run. To get all the songs, delete the `[:3]` part so that the line reads just `for page in songs_hrefs:`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Word Embeddings</h2>\n",
    "<br>\n",
    "The idea of embedding words is a bit like factor or principle component analysis; there are unobserved variables that capture the important features of the relationships between observations and if we can situate those observations in those dimensions, we can actually explore the relationships. In PCA or factor analysis you start with a high dimensional space defined by (although not necessarily identical to) the independent variables you have available. You then try to capture the variation present in a subset or linear combination of the variables. \n",
    "<br><br>\n",
    "In word embeddings, however, our units of analysis are words (roughly, more on that shortly). We don't have a bunch of variable values for each word, so we aren't going from a variable space to a new space with fewer variables. Instead, we are projecting words into a high dimensional space in hopes that our mechanism for determining the projection will create meaningful geometric relationships between words in those dimensions. The primary means of creating the projection is to look at the context around the given word. It is common to look at the 3 words on either side of the target word and use them to predict the target word or vise versa. The more similar the contexts of two different words, the closer they end up in the space. (The location of each word in the space is represented by a vector with a length equal to the number of dimensions choosen for the space. The value at each entry in the vector is that word's position for that dimension.)\n",
    "<br><br>\n",
    "The concept of word embeddings have been around for a while but made a big splash in 2013 with the introduction of the *word2vec* (words to vectors) embedding. Its method for determining the projection was significantly faster than previous versions, which allowed for many more tokens (word instances) to be processed. The first results reported relied on 1.6 *billion* tokens. It turned out that running more words through the algorithm improved the performance of the embedding compared to previous ones.\n",
    "<br><br>\n",
    "We don't have billions of words so we aren't going to *create* a word embedding of our own. (Presumably distinct corpuses have distinct relationships among the words so one might want to do that.) If you want to get a sense of what that looks like, the relevant code is at the bottom of this lab. Instead we're going to make use of an existing embedding from the GloVe (*Glo*bal *Ve*ctors for Word Representations). There are actually several different publicly available embeddings gone with the GloVe algorithm and we'll be using the smallest because the bigger ones require a lot of computer memory. \n",
    "<br><br>\n",
    "Below we'll load the 50 dimensional representation built using 6 billion words collected from Wikipedia and a corpus of new wire items (see [here]() for more information. It includes a vocabary of about 400k words. Again, what is included in this package is not raw text used to create an embedding. Instead we have a text file that contains the vector representations of the words. A 50 dimensional space is rather small--300 is common--but each one of those vectors would be 6 times as long and our file is already also 175MB. The 50 dimension space is good and will give you an idea of what working with an embedding is like, but it very likely less reliable than higher dimensional ones.\n",
    "<br><br>\n",
    "The first thing you'll need to do in order to continue is un-zip/extract the text file from the compressed version found in the `tools` folder inside of the folder this Notebook is in. The file is named `glove.6b.50d.txt.zip`. Double click on it or right click on it and open it with an archive utility. Once that is done you can start running the commands below.\n",
    "<br><br>\n",
    "First we import the package `pandas` so we can put the vectors into a data structure. We also need the `csv` package to tell `pandas` how to read the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data load failed. Chances are you didn't extract the text file as described above. Do that and run this cell again.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    words = pd.read_table(\"tools/glove.6B.50d.txt\", sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "    print(\"Vectors successfully loaded\")\n",
    "except:\n",
    "    print(\"Data load failed. Chances are you didn't extract the text file as described above. Do that and run this cell again.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"&7me\".isalnum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "Now that we have the data loaded, let's create some functions for analyzing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This takes a word and gives back the associated vector\n",
    "def get_vec(the_word):\n",
    "    clean_word = \"\".join([i for i in the_word.lower() if i.isalnum()])\n",
    "    return words.loc[clean_word].as_matrix()\n",
    "\n",
    "\n",
    "# This takes two words and finds the Cartesian distance between them, a simple way of assessing the similarity of the word\n",
    "def distance(word1, word2,verbose=False):\n",
    "    missed_count = 0\n",
    "    try:\n",
    "        vec_1 = get_vec(word1) # we put the words in lower case\n",
    "    except:\n",
    "        if verbose:\n",
    "            print(\"The first word is not in the embedding dictionary\")\n",
    "    try:\n",
    "        vec_2 = get_vec(word2)\n",
    "        return sum([(vec_1[i]-vec_2[i])**2 for i in range(len(vec_1))])**.5\n",
    "    except:\n",
    "        if verbose:\n",
    "            print('The second word is not in the embedding dictionary')\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "# This return average, max, min and standard deviation of the distance between the words in a song.\n",
    "def song_distances_summary(song,weighted=True,verbose=False):\n",
    "    flat_list = [item for sublist in song for item in sublist]\n",
    "    if not weighted:\n",
    "        # creating a set removes multiple instances of words and therefore doesn't weight by number of occurrences.\n",
    "        flat_list = list(set(song))\n",
    "        \n",
    "    distances = []\n",
    "    for i in range(len(song)):\n",
    "        for j in range(len(song)):\n",
    "            if i < j:\n",
    "                if song[i] != song[j]:\n",
    "                    dist = distance(song[i],song[j],verbose)\n",
    "                    if dist != None:\n",
    "                        distances.append(dist)\n",
    "                \n",
    "    max_d = max(distances)\n",
    "    min_d = min(distances)\n",
    "    \n",
    "    average = sum(distances)/len(distances)\n",
    "    \n",
    "    SD = (sum([(i-average)**2 for i in distances])/len(distances)-1)**.5\n",
    "    \n",
    "    return {\"average\":average, \"std_dev\":SD, \"max\":max_d, \"min\":min_d}\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a song we collected early and get a sense of spread of the words in the GloVe embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000-men\n"
     ]
    }
   ],
   "source": [
    "first_song = list(lyrics.keys())[1]\n",
    "print(first_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'average': 5.0107855761136237,\n",
       " 'max': 9.1707499130682884,\n",
       " 'min': 0.0,\n",
       " 'std_dev': 0.75532585939587604}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_distances_summary(lyrics[first_song],verbose=False)\n",
    "# remove the verbose parameter if you don't want to see at the missing word messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "#### Creating your own embedding ####\n",
    "Again, our Dylan lyrics collection is not nearly big enough to actually create an embedding that performs well. But we can still create one with the code below. We've cleaned everything so all the work is in the `gensim.model.Word2Vec` function. Running it with our puny corpus won't take long at all, but a suitably large corpus would need a lot more resources than your laptop. Thus this is just to show you how easy in is to create an embedding with existing python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.Word2Vec(lyrics, size=100, window=7, min_count=1)\n",
    "model.wv[\"just\"] # return the vector\n",
    "model.similarity(\"just\",\"living,\") # finds the distance between the words in the embedding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
